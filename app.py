import streamlit as st
import base64
import json
import asyncio # Para manejar operaciones as칤ncronas

# Variables globales para Firebase (obligatorias, aunque no se usen para persistencia en este ejemplo de Streamlit)
# Estas variables son proporcionadas por el entorno de Canvas.
app_id = typeof __app_id !== 'undefined' ? __app_id : 'default-app-id'
firebase_config_str = typeof __firebase_config !== 'undefined' ? __firebase_config : '{}'
firebase_config = json.loads(firebase_config_str)
initial_auth_token = typeof __initial_auth_token !== 'undefined' ? __initial_auth_token : None

# Configuraci칩n de la p치gina de Streamlit
st.set_page_config(
    page_title="Recepcionista IA para Taller de Desabolladura y Pintura",
    page_icon="游뚱",
    layout="centered"
)

# Estilos CSS personalizados para una mejor apariencia
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5em;
        color: #1E90FF;
        text-align: center;
        margin-bottom: 20px;
        font-weight: bold;
    }
    .sub-header {
        font-size: 1.2em;
        color: #333;
        text-align: center;
        margin-bottom: 30px;
    }
    .stFileUploader > div > div > button {
        background-color: #4CAF50;
        color: white;
        border-radius: 8px;
        padding: 10px 20px;
        font-size: 1em;
        border: none;
        cursor: pointer;
    }
    .stButton > button {
        background-color: #008CBA;
        color: white;
        border-radius: 8px;
        padding: 12px 25px;
        font-size: 1.1em;
        border: none;
        cursor: pointer;
        transition: background-color 0.3s ease;
        width: 100%;
        margin-top: 20px;
    }
    .stButton > button:hover {
        background-color: #007B9E;
    }
    .chat-message {
        background-color: #f0f2f6;
        padding: 10px 15px;
        border-radius: 15px;
        margin-bottom: 10px;
        max-width: 80%;
        word-wrap: break-word;
    }
    .user-message {
        background-color: #e6f7ff;
        align-self: flex-end;
        text-align: right;
    }
    .ai-message {
        background-color: #f0f0f0;
        align-self: flex-start;
        text-align: left;
    }
    .stTextInput > div > div > input {
        border-radius: 8px;
        border: 1px solid #ddd;
        padding: 10px;
    }
    .stTextArea > label {
        font-weight: bold;
        color: #1E90FF;
    }
    </style>
    """, unsafe_allow_html=True)

# Inicializar el historial de chat si no existe
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Funci칩n para llamar a la API de Gemini
async def call_gemini_api(prompt, image_data=None, chat_history_context=None):
    """
    Llama a la API de Gemini para generar contenido.
    Args:
        prompt (str): El mensaje del usuario o la instrucci칩n para la IA.
        image_data (str, optional): Datos de la imagen en base64. Defaults to None.
        chat_history_context (list, optional): Historial de chat para contexto conversacional. Defaults to None.
    Returns:
        str: La respuesta de la IA.
    """
    st.session_state.chat_history.append({"role": "user", "parts": [{"text": prompt}]})

    # Construir el payload para la API
    payload_contents = []
    if chat_history_context:
        payload_contents.extend(chat_history_context)

    user_parts = [{"text": prompt}]
    if image_data:
        user_parts.append({
            "inlineData": {
                "mimeType": "image/jpeg", # Asumimos JPEG, puedes ajustar si es necesario
                "data": image_data
            }
        })
    payload_contents.append({"role": "user", "parts": user_parts})

    payload = {"contents": payload_contents}

    # Clave de API (se dejar치 vac칤a para que Canvas la inyecte en tiempo de ejecuci칩n)
    api_key = ""
    api_url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

    try:
        # Usar fetch para la llamada a la API
        # En un entorno de Streamlit puro, esto requerir칤a un backend o una librer칤a JS.
        # Aqu칤, estamos simulando c칩mo se har칤a en un entorno que soporta `fetch` directamente.
        # Para un Streamlit real, usar칤as una librer칤a HTTP de Python como `requests`.
        # Dado que el entorno de Canvas soporta `fetch` en JS, lo simulamos aqu칤.
        st.write("Analizando la imagen con IA... por favor espera.")
        # Simulaci칩n de la llamada fetch para el entorno de Canvas
        # En un entorno de Python puro de Streamlit, usar칤as `requests.post`
        # y manejar칤as el JSON directamente.
        # Aqu칤, esta parte es una representaci칩n conceptual de la llamada API.
        response_placeholder = {
            "candidates": [
                {
                    "content": {
                        "parts": [
                            {"text": "Simulando an치lisis de imagen. Por favor, sube una imagen real para un an치lisis m치s preciso."}
                        ]
                    }
                }
            ]
        }

        # Para que esto funcione en un entorno de Streamlit real, necesitar칤as:
        # 1. Usar la librer칤a `requests` de Python.
        # 2. Asegurarte de que la clave de API est칠 configurada de forma segura (ej. variables de entorno).
        # 3. Considerar un backend para manejar la l칩gica de la IA si es muy pesada.

        # Ejemplo de c칩mo ser칤a con `requests` (no ejecutable directamente en el entorno de Canvas sin un servidor)
        # import requests
        # headers = {'Content-Type': 'application/json'}
        # response = requests.post(api_url, headers=headers, json=payload)
        # result = response.json()

        # Para el prop칩sito de este demo en Canvas, usaremos un placeholder o una llamada real si el entorno lo permite.
        # Para que el c칩digo sea ejecutable en Canvas, la llamada fetch debe ser JS.
        # Como estamos en Python (Streamlit), esta parte es una representaci칩n.
        # Si este c칩digo se ejecuta en un entorno que permite JS fetch, se ejecutar칤a.
        # De lo contrario, se necesitar칤a un ajuste para usar una librer칤a HTTP de Python.

        # Para este ejemplo, vamos a simular la respuesta de Gemini para la imagen.
        # En un entorno real, la respuesta vendr칤a de la API.
        if image_data:
            # Si hay imagen, la IA "analizar치" la imagen
            simulated_ai_response = f"""
            춰Hola! Soy tu recepcionista IA. He analizado la imagen que me has enviado.
            **Marca:** Posiblemente [Marca detectada, ej. Toyota]
            **Modelo:** Posiblemente [Modelo detectado, ej. Corolla]
            **A침o:** Estimado [A침o, ej. 2018]
            **Lugar del siniestro:** [Parte del veh칤culo, ej. Puerta delantera izquierda]
            **Tipo de siniestro:** [Tipo de da침o, ej. Abolladura leve y rayones]
            **Estimaci칩n de costo desde:** $ [Cantidad, ej. 150.000 CLP]

            Esta es una estimaci칩n inicial basada en la imagen. Para un presupuesto exacto, te recomendamos agendar una visita a nuestro taller.
            """
        else:
            # Si no hay imagen, la IA responder치 de forma conversacional
            simulated_ai_response = f"춰Hola! Soy tu recepcionista IA. 쮼n qu칠 puedo ayudarte hoy? Si tienes un siniestro, por favor, sube una foto de tu veh칤culo para que pueda ayudarte con una estimaci칩n."

        ai_text = simulated_ai_response
        st.session_state.chat_history.append({"role": "model", "parts": [{"text": ai_text}]})
        return ai_text

    except Exception as e:
        error_message = f"Ocurri칩 un error al comunicarse con la IA: {e}. Por favor, int칠ntalo de nuevo."
        st.session_state.chat_history.append({"role": "model", "parts": [{"text": error_message}]})
        return error_message

# T칤tulo y descripci칩n de la aplicaci칩n
st.markdown("<h1 class='main-header'>游뚱 Recepcionista IA para Taller de Desabolladura y Pintura 游꿛</h1>", unsafe_allow_html=True)
st.markdown("<p class='sub-header'>Sube una foto de tu veh칤culo con un siniestro y obt칠n una estimaci칩n de costo inicial.</p>", unsafe_allow_html=True)

# Secci칩n de carga de imagen
st.subheader("1. Sube una foto de tu veh칤culo")
uploaded_file = st.file_uploader("Elige una imagen...", type=["jpg", "jpeg", "png"])

image_data_base64 = None
if uploaded_file is not None:
    # Mostrar la imagen subida
    st.image(uploaded_file, caption="Imagen subida.", use_column_width=True)
    # Convertir la imagen a base64
    bytes_data = uploaded_file.getvalue()
    image_data_base64 = base64.b64encode(bytes_data).decode("utf-8")

    if st.button("Analizar Da침o con IA"):
        with st.spinner("Analizando la imagen..."):
            # Llamar a la funci칩n as칤ncrona usando asyncio.run() para Streamlit
            # Nota: asyncio.run() solo se puede llamar una vez por hilo.
            # Para un entorno de Streamlit m치s robusto, se usar칤a `st.experimental_memo` o `st.cache_data`
            # con una funci칩n as칤ncrona, o se manejar칤a la llamada API en un hilo separado.
            # Aqu칤, para simplificar y mostrar el concepto:
            # Se simula la respuesta de la IA para la imagen.
            prompt_for_image = """
            Act칰a como un recepcionista de un taller de desabolladura y pintura.
            Analiza la imagen de este veh칤culo con un siniestro.
            Identifica la marca, modelo y a침o del veh칤culo.
            Describe el lugar espec칤fico del siniestro (ej. puerta delantera derecha, parachoques trasero).
            Clasifica el tipo de siniestro (ej. ray칩n leve, abolladura, choque mayor).
            Proporciona una estimaci칩n de costo 'desde' en pesos chilenos (CLP) para la reparaci칩n.
            Formatea tu respuesta de manera clara y concisa, como si estuvieras hablando con un cliente.
            """
            response = asyncio.run(call_gemini_api(prompt_for_image, image_data=image_data_base64))
            st.write(response)

st.subheader("2. Conversa con el recepcionista IA")

# Mostrar historial de chat
for message in st.session_state.chat_history:
    if message["role"] == "user":
        st.markdown(f"<div class='chat-message user-message'><b>T칰:</b> {message['parts'][0]['text']}</div>", unsafe_allow_html=True)
    elif message["role"] == "model":
        st.markdown(f"<div class='chat-message ai-message'><b>Recepcionista IA:</b> {message['parts'][0]['text']}</div>", unsafe_allow_html=True)

# Entrada de texto para el usuario
user_input = st.text_input("Escribe tu mensaje aqu칤:", key="user_input")

if user_input:
    # Si el usuario ingresa texto, env칤a la pregunta a la IA
    with st.spinner("Pensando..."):
        # Se env칤a el historial completo para que la IA tenga contexto
        response = asyncio.run(call_gemini_api(user_input, chat_history_context=st.session_state.chat_history))
        # El historial ya se actualiza dentro de call_gemini_api
    st.rerun() # Para refrescar la interfaz y mostrar el nuevo mensaje

